import os
from typing import Optional, List, Dict, Any
from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic
import sys
import os
import asyncio

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from api_server.client.mcp_client import get_mcp_client,get_mcp_client_async

load_dotenv()  # load environment variables from .env

# mcp_client = get_mcp_client_async()
tools = None
model_type = os.getenv('MODEL_TYPE', 'openai')

# Traffic-R1 æ¨¡å‹ç›¸å…³å¯¼å…¥
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ transformers/torch æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ Traffic-R1 æ¨¡å‹")

class LLMClient:
    def __init__(self, model_type: str = model_type):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        Args:
            model_type: æ¨¡å‹ç±»å‹ï¼Œå¯é€‰å€¼ï¼šopenai, anthropic, siliconflow, traffic_r1, local_gguf, lm-studio
        """
        self.model_type = model_type
        self.client = None
        self.model = None  # ç”¨äº transformers æ¨¡å‹
        self.tokenizer = None  # ç”¨äº transformers tokenizer
        self._init_client()
        self.mcp_client = None
        self.tools = []
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–MCPå®¢æˆ·ç«¯"""
        print("è°ƒç”¨MCPè¿æ¥")
        self.mcp_client = await get_mcp_client_async()
        
    def _init_client(self):
        """åˆå§‹åŒ–å¯¹åº”çš„å®¢æˆ·ç«¯"""
        if self.model_type == "openai":
            self.client = OpenAI(
                api_key=os.getenv('DASHSCOPE_API_KEY'),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        elif self.model_type == "anthropic":
            self.client = Anthropic()
        elif self.model_type == "siliconflow":
            self.client = OpenAI(
                api_key=os.getenv('SILICONFLOW_API_KEY', ''),
                base_url=os.getenv('SILICONFLOW_BASE_URL', 'https://api.siliconflow.cn/v1')
            )
        elif self.model_type == "traffic_r1":
            # Traffic-R1 æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–
            if not TRANSFORMERS_AVAILABLE:
                raise ImportError("âŒ éœ€è¦å®‰è£… transformers å’Œ torch æ‰èƒ½ä½¿ç”¨ Traffic-R1 æ¨¡å‹")
            
            model_path = os.getenv('TRAFFIC_R1_MODEL_PATH', './models/Traffic-R1')
            print(f"ğŸš€ æ­£åœ¨åŠ è½½ Traffic-R1 æ¨¡å‹: {model_path}")
            
            # æ£€æµ‹è®¾å¤‡
            if torch.backends.mps.is_available():
                device = "mps"  # Apple Silicon
                print("âœ… ä½¿ç”¨ MPS (Apple Silicon) åŠ é€Ÿ")
            elif torch.cuda.is_available():
                device = "cuda"
                print("âœ… ä½¿ç”¨ CUDA åŠ é€Ÿ")
            else:
                device = "cpu"
                print("âš ï¸ ä½¿ç”¨ CPU è¿è¡Œï¼ˆè¾ƒæ…¢ï¼‰")
            
            # åŠ è½½æ¨¡å‹å’Œtokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16 if device != "cpu" else torch.float32,
                device_map=device if device != "mps" else None,  # MPSä¸æ”¯æŒdevice_map
                trust_remote_code=True
            )
            
            # å¦‚æœæ˜¯MPSï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹
            if device == "mps":
                self.model = self.model.to(device)
            
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            print(f"âœ… Traffic-R1 æ¨¡å‹åŠ è½½æˆåŠŸï¼è®¾å¤‡: {device}")
            
        elif self.model_type == "local_gguf":
            # GGUFæ ¼å¼æ¨¡å‹åˆå§‹åŒ–
            from llama_cpp import Llama
            self.client = Llama(
                model_path=os.getenv('LOCAL_GGUF_MODEL_PATH', './models/qwen3-8b.Q4_K_M.gguf'),
                n_ctx=4096,
                n_gpu_layers=1,  # MetalåŠ é€Ÿ
                verbose=False
            )
        elif self.model_type == "local" or self.model_type == "lm-studio":
            # æœ¬åœ°LM-Studioæ¨¡å¼
            self.client = OpenAI(
                api_key=os.getenv('LM_STUDIO_API_KEY', 'lm-studio'),
                base_url=os.getenv('LM_STUDIO_BASE_URL', 'http://127.0.0.1:1234/v1')
            )
            print(f"âœ… æœ¬åœ°LM-Studioå·²è¿æ¥: {self.client.base_url}")
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        
        

    def chat(self, messages: List[Dict[str, str]], 
             temperature: float = 0.3,
             max_tokens: int = 100,
             model: str = None) -> Dict[str, Any]:
        """
        åŒæ­¥æ–¹æ³•ï¼šè·å–æ¨¡å‹å›å¤ï¼ˆä¸ä½¿ç”¨æµå¼è¾“å‡ºï¼Œç”¨äºä¿¡å·æ§åˆ¶ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            model: æ¨¡å‹åç§°
        
        Returns:
            åŒ…å«contentçš„å­—å…¸
        """
        model = model or os.getenv('MODEL_NAME', 'qwen-plus')
        
        try:
            if self.model_type == "openai":
                response = self.client.chat.completions.create(
                    temperature=temperature,
                    model=model,
                    messages=messages,
                    stream=False,
                    max_tokens=max_tokens
                )
                return {"content": response.choices[0].message.content}
                
            elif self.model_type == "anthropic":
                response = self.client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=max_tokens,
                    messages=messages,
                    temperature=temperature
                )
                return {"content": response.content[0].text}
                
            elif self.model_type == "siliconflow":
                response = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    stream=False,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return {"content": response.choices[0].message.content}
                
            elif self.model_type == "traffic_r1":
                # Traffic-R1 æ¨¡å‹æ¨ç†
                # æ„å»ºè¾“å…¥æ–‡æœ¬
                text = self._format_messages_for_traffic_r1(messages)
                
                # Tokenize
                inputs = self.tokenizer(text, return_tensors="pt")
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                device = next(self.model.parameters()).device
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        temperature=temperature,
                        do_sample=temperature > 0,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç 
                response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # å»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å†…å®¹
                if text in response_text:
                    response_text = response_text[len(text):].strip()
                
                return {"content": response_text}
                
            elif self.model_type == "local" or self.model_type == "lm-studio":
                response = self.client.chat.completions.create(
                    model=model or "local-model",
                    messages=messages,
                    stream=False,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return {"content": response.choices[0].message.content}
            else:
                raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
                
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _format_messages_for_traffic_r1(self, messages: List[Dict[str, str]]) -> str:
        """
        æ ¼å¼åŒ–æ¶ˆæ¯ä¸º Traffic-R1 æ¨¡å‹çš„è¾“å…¥æ ¼å¼
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        # Qwen2 æ ¼å¼çš„å¯¹è¯æ¨¡æ¿
        formatted_text = ""
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                formatted_text += f"<|im_start|>system\n{content}<|im_end|>\n"
            elif role == "user":
                formatted_text += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                formatted_text += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        
        # æ·»åŠ æœ€åçš„ assistant æ ‡è®°ï¼Œç­‰å¾…æ¨¡å‹ç”Ÿæˆ
        formatted_text += "<|im_start|>assistant\n"
        
        return formatted_text

    async def get_chat_response(self, 
                           messages: List[Dict[str, str]], 
                           model: str = os.getenv('MODEL_NAME'),
                           stream: bool = True,
                           tools: Optional[List[Dict[str, Any]]] = tools
                           ) -> Any:
        """
        å¼‚æ­¥æ–¹æ³•ï¼šè·å–æ¨¡å‹å›å¤ï¼ˆæµå¼è¾“å‡ºï¼Œç”¨äºå¯¹è¯ï¼‰
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
        Returns:
            æ¨¡å‹å›å¤
        """
        await self.initialize()
        tools = await self.mcp_client.get_list_tools()
        if self.model_type == "openai":
            return self.client.chat.completions.create(
                temperature=0.5,
                model=model,
                messages=messages,
                stream=stream,
                tools=tools
            )
        elif self.model_type == "anthropic":
            return self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                messages=messages
            )
        elif self.model_type == "siliconflow":
            return self.client.chat.completions.create(
                model=model,
                messages=messages,
                stream=stream,
                tools=tools
            )
        elif self.model_type == "local" or self.model_type == "lm-studio":
            return self.client.chat.completions.create(
                model=model or "local-model",
                messages=messages,
                stream=stream,
                tools=tools
            )
        else:
            raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
        

if __name__ == "__main__":
    mcp_client = get_mcp_client()
    print(mcp_client.sync_list_tools())